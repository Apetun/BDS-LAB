{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|     John Doe|\n",
      "|     john doe|\n",
      "|       J. Doe|\n",
      "|Johnathan Doe|\n",
      "|      Jon Doe|\n",
      "|     Jhon Doe|\n",
      "| Mr. John Doe|\n",
      "|   Jane Smith|\n",
      "|     J. Smith|\n",
      "|Jane A. Smith|\n",
      "|  Smith, Jane|\n",
      "|   J.A. Smith|\n",
      "|    Mr. Smith|\n",
      "|  Janes Smyth|\n",
      "|  Jane Smythe|\n",
      "+-------------+\n",
      "\n",
      "Cleaned DataFrame:\n",
      "+-------------+-------------+\n",
      "|         name| cleaned_name|\n",
      "+-------------+-------------+\n",
      "|     John Doe|     john doe|\n",
      "|     john doe|     john doe|\n",
      "|       J. Doe|        j doe|\n",
      "|Johnathan Doe|johnathan doe|\n",
      "|      Jon Doe|      jon doe|\n",
      "|     Jhon Doe|     jhon doe|\n",
      "| Mr. John Doe|  mr john doe|\n",
      "|   Jane Smith|   jane smith|\n",
      "|     J. Smith|      j smith|\n",
      "|Jane A. Smith| jane a smith|\n",
      "|  Smith, Jane|   smith jane|\n",
      "|   J.A. Smith|     ja smith|\n",
      "|    Mr. Smith|     mr smith|\n",
      "|  Janes Smyth|  janes smyth|\n",
      "|  Jane Smythe|  jane smythe|\n",
      "+-------------+-------------+\n",
      "\n",
      "Tokenized DataFrame:\n",
      "+-------------+-------------+----------------+\n",
      "|name         |cleaned_name |tokens          |\n",
      "+-------------+-------------+----------------+\n",
      "|John Doe     |john doe     |[john, doe]     |\n",
      "|john doe     |john doe     |[john, doe]     |\n",
      "|J. Doe       |j doe        |[j, doe]        |\n",
      "|Johnathan Doe|johnathan doe|[johnathan, doe]|\n",
      "|Jon Doe      |jon doe      |[jon, doe]      |\n",
      "|Jhon Doe     |jhon doe     |[jhon, doe]     |\n",
      "|Mr. John Doe |mr john doe  |[mr, john, doe] |\n",
      "|Jane Smith   |jane smith   |[jane, smith]   |\n",
      "|J. Smith     |j smith      |[j, smith]      |\n",
      "|Jane A. Smith|jane a smith |[jane, a, smith]|\n",
      "|Smith, Jane  |smith jane   |[smith, jane]   |\n",
      "|J.A. Smith   |ja smith     |[ja, smith]     |\n",
      "|Mr. Smith    |mr smith     |[mr, smith]     |\n",
      "|Janes Smyth  |janes smyth  |[janes, smyth]  |\n",
      "|Jane Smythe  |jane smythe  |[jane, smythe]  |\n",
      "+-------------+-------------+----------------+\n",
      "\n",
      "Filtered Tokens DataFrame:\n",
      "+-------------+-------------+----------------+----------------+\n",
      "|name         |cleaned_name |tokens          |filtered_tokens |\n",
      "+-------------+-------------+----------------+----------------+\n",
      "|John Doe     |john doe     |[john, doe]     |[john, doe]     |\n",
      "|john doe     |john doe     |[john, doe]     |[john, doe]     |\n",
      "|J. Doe       |j doe        |[j, doe]        |[j, doe]        |\n",
      "|Johnathan Doe|johnathan doe|[johnathan, doe]|[johnathan, doe]|\n",
      "|Jon Doe      |jon doe      |[jon, doe]      |[jon, doe]      |\n",
      "|Jhon Doe     |jhon doe     |[jhon, doe]     |[jhon, doe]     |\n",
      "|Mr. John Doe |mr john doe  |[mr, john, doe] |[mr, john, doe] |\n",
      "|Jane Smith   |jane smith   |[jane, smith]   |[jane, smith]   |\n",
      "|J. Smith     |j smith      |[j, smith]      |[j, smith]      |\n",
      "|Jane A. Smith|jane a smith |[jane, a, smith]|[jane, smith]   |\n",
      "|Smith, Jane  |smith jane   |[smith, jane]   |[smith, jane]   |\n",
      "|J.A. Smith   |ja smith     |[ja, smith]     |[ja, smith]     |\n",
      "|Mr. Smith    |mr smith     |[mr, smith]     |[mr, smith]     |\n",
      "|Janes Smyth  |janes smyth  |[janes, smyth]  |[janes, smyth]  |\n",
      "|Jane Smythe  |jane smythe  |[jane, smythe]  |[jane, smythe]  |\n",
      "+-------------+-------------+----------------+----------------+\n",
      "\n",
      "Normalized DataFrame:\n",
      "+-------------+-------------+----------------+----------------+---------------+\n",
      "|name         |cleaned_name |tokens          |filtered_tokens |normalized_name|\n",
      "+-------------+-------------+----------------+----------------+---------------+\n",
      "|John Doe     |john doe     |[john, doe]     |[john, doe]     |john doe       |\n",
      "|john doe     |john doe     |[john, doe]     |[john, doe]     |john doe       |\n",
      "|J. Doe       |j doe        |[j, doe]        |[j, doe]        |j doe          |\n",
      "|Johnathan Doe|johnathan doe|[johnathan, doe]|[johnathan, doe]|johnathan doe  |\n",
      "|Jon Doe      |jon doe      |[jon, doe]      |[jon, doe]      |jon doe        |\n",
      "|Jhon Doe     |jhon doe     |[jhon, doe]     |[jhon, doe]     |jhon doe       |\n",
      "|Mr. John Doe |mr john doe  |[mr, john, doe] |[mr, john, doe] |mr john doe    |\n",
      "|Jane Smith   |jane smith   |[jane, smith]   |[jane, smith]   |jane smith     |\n",
      "|J. Smith     |j smith      |[j, smith]      |[j, smith]      |j smith        |\n",
      "|Jane A. Smith|jane a smith |[jane, a, smith]|[jane, smith]   |jane smith     |\n",
      "|Smith, Jane  |smith jane   |[smith, jane]   |[smith, jane]   |smith jane     |\n",
      "|J.A. Smith   |ja smith     |[ja, smith]     |[ja, smith]     |ja smith       |\n",
      "|Mr. Smith    |mr smith     |[mr, smith]     |[mr, smith]     |mr smith       |\n",
      "|Janes Smyth  |janes smyth  |[janes, smyth]  |[janes, smyth]  |janes smyth    |\n",
      "|Jane Smythe  |jane smythe  |[jane, smythe]  |[jane, smythe]  |jane smythe    |\n",
      "+-------------+-------------+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, trim, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EntityResolutionPreprocessing\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "df=spark.read.csv('sample_entities.csv',header=True,inferSchema=True)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Step 1: Data Cleaning\n",
    "df_cleaned = df.withColumn(\"cleaned_name\", lower(col(\"name\"))) \\\n",
    "               .withColumn(\"cleaned_name\", regexp_replace(col(\"cleaned_name\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
    "               .withColumn(\"cleaned_name\", trim(col(\"cleaned_name\")))\n",
    "\n",
    "print(\"Cleaned DataFrame:\")\n",
    "df_cleaned.show()\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_name\", outputCol=\"tokens\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)\n",
    "\n",
    "print(\"Tokenized DataFrame:\")\n",
    "df_tokenized.show(truncate=False)\n",
    "\n",
    "# Step 3: Remove Stop Words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "df_filtered = stopwords_remover.transform(df_tokenized)\n",
    "\n",
    "print(\"Filtered Tokens DataFrame:\")\n",
    "df_filtered.show(truncate=False)\n",
    "\n",
    "# Step 4: Normalization\n",
    "df_normalized = df_filtered.withColumn(\"normalized_name\", concat_ws(\" \", col(\"filtered_tokens\")))\n",
    "\n",
    "print(\"Normalized DataFrame:\")\n",
    "df_normalized.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
